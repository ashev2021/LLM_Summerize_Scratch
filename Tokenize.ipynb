{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3ostkZSo3kZm1RIHGaqR3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashev2021/LLM_Summerize_Scratch/blob/main/Tokenize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-G_hRAjlP8X4"
      },
      "outputs": [],
      "source": [
        "##Tokenize Class\n",
        "import re\n",
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Split text into tokens (words + punctuation)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "\n",
        "tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "tokens = [item.strip() for item in tokens if item.strip()]\n",
        "\n",
        "# Step 2: Build vocab dictionary\n",
        "all_words = sorted(set(tokens))\n",
        "vocab = {token: idx for idx, token in enumerate(all_words)}\n",
        "\n",
        "\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "print(len(all_words))"
      ],
      "metadata": {
        "id": "3KV4YlivP_OO",
        "outputId": "d1880065-d94c-4fec-a2ad-ea386f14f5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 5, 1, 13, 15, 9, 7, 10, 2, 17, 8, 2, 0, 6, 3, 4, 14, 16, 11, 12, 3]\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Decode\n",
        "decoded_text = tokenizer.decode(ids)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "id": "Cp0XiW20Qro3",
        "outputId": "fb0e6327-64fb-48a4-990b-215a11e18997",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted((set(tokens)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "\n",
        "print(len(all_tokens))"
      ],
      "metadata": {
        "id": "rEbJmbafR8Zk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92e30e4-c7f2-4a4c-98ab-8c43898bf197"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "           \"the-verdict.txt\")\n",
        "    file_path = \"the-verdict.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)"
      ],
      "metadata": {
        "id": "kJundHHSGM2O"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wipeyOw4HRWm",
        "outputId": "f8880a12-e9a1-4da5-8885-6cf1cadf71d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_new_tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "tokens = [item.strip() for item in tokens if item.strip()]\n",
        "\n",
        "# Step 2: Build vocab dictionary\n",
        "all_words = sorted(set(all_new_tokens))\n",
        "vocab_new = {token: idx for idx, token in enumerate(all_words)}\n",
        "print(len(vocab_new))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-qC4McmHUGn",
        "outputId": "85b7dc66-e344-4661-ca82-6b51aac3392d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted((set(all_new_tokens)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab_update = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "\n",
        "print(len(all_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRhGwhH2HtpR",
        "outputId": "4c844ff4-7754-4efa-a555-da48f73c9b00"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab_update.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdmBScyMIE6l",
        "outputId": "0311ee72-00d3-429b-f1cb-2592d2400495"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1130)\n",
            "('your', 1131)\n",
            "('yourself', 1132)\n",
            "('<|endoftext|>', 1133)\n",
            "('<|unk|>', 1134)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#class updated adding \"unk\", \"endoftext\"\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "59XuG0pzIbNL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "raw_text2 = \"Arezo, \" +\"It's the last he painted, you know,\" + \"<|endoftext|>\" + raw_text\n",
        "\n",
        "#print(raw_text2)\n",
        "\n",
        "all_new_tokens2 = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text2)\n",
        "tokens2 = [item.strip() for item in tokens if item.strip()]\n",
        "\n",
        "# Step 2: Build vocab dictionary\n",
        "all_words2 = sorted(set(all_new_tokens2))\n",
        "vocab_new2 = {token: idx for idx, token in enumerate(all_words2)}\n",
        "\n",
        "\n",
        "tokenizer = SimpleTokenizerV2(vocab_new2)\n",
        "\n",
        "print(tokenizer.encode(raw_text2)[:99])\n",
        "print(tokenizer.decode(tokenizer.encode(raw_text2)[:99]))\n",
        "\n",
        "#print(len(vocab_new2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0IWjRReMJ4_",
        "outputId": "8a9f4b45-fed0-4dd5-f6e0-f622d883fd28"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 8, 61, 5, 855, 993, 607, 538, 751, 8, 1131, 601, 8, 13, 49, 154, 1008, 62, 43, 823, 120, 261, 491, 9, 1007, 120, 505, 440, 397, 9, 913, 590, 1082, 714, 513, 966, 1021, 668, 1021, 540, 992, 8, 573, 993, 543, 727, 554, 501, 8, 538, 519, 375, 554, 753, 8, 666, 120, 846, 1107, 8, 162, 402, 552, 573, 120, 1071, 732, 993, 89, 10, 6, 104, 58, 823, 1008, 590, 1125, 535, 213, 90, 739, 39, 10, 7, 4, 98, 543, 727, 554, 501, 4, 9, 992, 1082, 1094, 993, 1117, 247, 590]\n",
            "Arezo, It' s the last he painted, you know, <|endoftext|>I HAD always thought Jack Gisburn rather a cheap genius -- though a good fellow enough -- so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera.( Though I rather thought it would have been Rome or Florence.)\" The height of his glory\" -- that was what the women called it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BPE encoding\n",
        "\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbfOiPAzMRHl",
        "outputId": "571864d9-21d4-467d-9810-2016ce72b669"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxljx_wmWFwJ",
        "outputId": "b19757b3-dd3e-4e4c-bf4c-60f0dc51bc2f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_BPE = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "integers = tokenizer_BPE.encode(raw_text2, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "#print(integers)"
      ],
      "metadata": {
        "id": "hvu2eeMtWWQb"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer_BPE.decode(integers)\n",
        "\n",
        "print(strings[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ_Gb1ekWdFv",
        "outputId": "70bfc4f2-377e-40f3-e59d-1cd45f1a213e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arezo, It's the last he painted, you know,<|endoftext|>I HAD always thought Jack Gisburn rather a c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sliding window (Data sampling)\n",
        "\n"
      ],
      "metadata": {
        "id": "fmWLb1QVW_0W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}